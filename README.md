# Web Scraper Project

Этот проект представляет собой асинхронный веб-скрапер на базе FastAPI, использующий библиотеку httpx для сбора данных. Инструмент поддерживает работу c статическими сайтами, с динамическими сайтами(маркетплейсы) не работает, нужна доработка.

## Описание проекта

Скрапер выполняет многоуровневый обход веб-страниц (BFS), извлекая заголовки и URL-адреса. Данные сохраняются в базу данных для последующего использования через API.

## Структура проекта

- app/main.py - точка входа в приложение FastAPI, где описаны эндпоинты и логика обработки запросов.
- service/scraper.py - основной скрипт скрапера, содержащий логику обхода, фильтрации и взаимодействия с браузером.
- db/ - директория для работы с базой данных, включая модели SQLAlchemy и настройки асинхронной сессии.
- requirements.txt - список зависимостей для установки окружения.
- .env - файл для хранения конфигурационных данных (база данных, лимиты).

## Конфигурационные файлы

Для гибкой настройки работы скрапера используются следующие текстовые файлы:

- blocked_sites.txt: содержит список URL-адресов или доменов, которые скраперу запрещено посещать. Если ссылка начинается с одной из строк в этом файле, она будет проигнорирована. Это полезно для исключения рекламных сервисов, соцсетей или ресурсов, доступ к которым ограничен.
- ignored_ext.txt: список расширений файлов, которые не являются веб-страницами (например, .jpg, .pdf, .zip, .mp4). Проверка по этому списку предотвращает попытки скрапера скачать тяжелые бинарные данные, что экономит оперативную память и интернет-трафик.
- description.json: файл в формате JSON, где хранятся текстовые описания для методов API. Приложение считывает эти данные при запуске и отображает их в документации Swagger (/docs), что позволяет не загромождать программный код длинными строками текста.

## Важное предупреждение о глубине поиска!

При работе со скрапером необходимо учитывать следующее:

1. Ресурсные затраты: С увеличением параметра глубины (depth) количество найденных и обрабатываемых ссылок растет в геометрической прогрессии. На глубине более 2-3 на крупных порталах количество задач может исчисляться дястками тысяч, а на 5-ой и все полмиллиона, что требует значительного объема оперативной памяти.
2. Валидация: В коде FastAPI установлена жесткая валидация параметра depth. По умолчанию максимальное значение составляет 6.
3. Изменение лимитов: Если вам требуется большая глубина, вы можете изменить значение аргумента le (less or equal) в определении маршрута (Query). Однако делайте это с осторожностью, чтобы избежать перегрузки системы.

## API Методы

Основные эндпоинты приложения:

- GET /get_list: Возвращает список всех найденных ссылок и их заголовков из базы данных. Описание метода загружается из файла description.json.
- POST /post_data: Принимает URL для начала скрапинга.
    - Аргументы:
        - url (string): начальная точка скрапинга.
        - depth (int): глубина обхода (валидация от 0 до 6).
        - max_concurrency (int): количество одновременно обрабатываемых ссылок.
- GET /search: Поиск HTML тела по одному из двух аргументов.
    - Аргументы:
        - url (string): поиску по url-ссылке.
        - title (int): поиск по тегу title.


## Установка и запуск(docker)

1. Создание файла окружения:
Перед началом работы создайте файл ".env" c следующими параметрами:
    -DB_HOST=db
    -DB_PORT=5432
    -DB_USER='ВАШ ЮЗЕРНЕЙМ В БД'
    -DB_PASS='ВАШ ПАРОЛЬ В БД'
    -DB_NAME=web_scraper

2. Сборка образа:
docker-compose build

2. Запуск контейнеров:
docker-compose up -d

При запуске через Docker приложение будет доступно по адресу http://localhost:8000. Также доступна SwagerUI по эндпоинту /docs

